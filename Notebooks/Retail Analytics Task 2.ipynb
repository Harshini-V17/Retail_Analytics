{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8af1a65-07e0-478a-9408-59182ffbe5e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.mv(\"dbfs:/FileStore/tables/store_sales.csv\", \"dbfs:/FileStore/data_pipeline/raw_data/store_sales.csv\")\n",
    "dbutils.fs.mv(\"dbfs:/FileStore/tables/products.csv\", \"dbfs:/FileStore/data_pipeline/raw_data/products.csv\")\n",
    "dbutils.fs.mv(\"dbfs:/FileStore/tables/stores.csv\", \"dbfs:/FileStore/data_pipeline/raw_data/stores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0828128-e493-4871-81c9-dbc55ae4129e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_message(message):\n",
    "    log_path = \"dbfs:/FileStore/data_pipeline/logs/log.txt\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    full_msg = f\"[{timestamp}] {message}\\n\"\n",
    "    dbutils.fs.put(log_path, full_msg, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f5eb2e2-b0fb-4f2a-aaa5-8d750d1b0afa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_message(message):\n",
    "    log_path = \"dbfs:/FileStore/data_pipeline/logs/log.txt\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_entry = f\"[{timestamp}] {message}\\n\"\n",
    "    \n",
    "    try:\n",
    "        # Try to read existing content\n",
    "        existing = dbutils.fs.head(log_path, 1000000)\n",
    "    except:\n",
    "        existing = \"\"\n",
    "    \n",
    "    # Append new log entry\n",
    "    dbutils.fs.put(log_path, existing + log_entry, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36f762f8-bb39-471d-bf25-6c55d2910ff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 489 bytes.\nWrote 550 bytes.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, col, lower, trim, expr\n",
    "\n",
    "try:\n",
    "    log_message(\"Transformation started.\")\n",
    "    \n",
    "    # Clean\n",
    "    sales_df = sales_df.withColumn(\"sale_date\", to_date(col(\"sale_date\")))\n",
    "    sales_df = sales_df.withColumn(\"quantity\", col(\"quantity\").cast(\"int\"))\n",
    "    sales_df = sales_df.withColumn(\"unit_price\", col(\"unit_price\").cast(\"double\"))\n",
    "    sales_df = sales_df.withColumn(\"discount\", col(\"discount\").cast(\"double\"))\n",
    "    products_df = products_df.withColumn(\"cost_price\", col(\"cost_price\").cast(\"double\"))\n",
    "    products_df = products_df.withColumn(\"category\", lower(trim(col(\"category\"))))\n",
    "    stores_df = stores_df.withColumn(\"region\", lower(trim(col(\"region\"))))\n",
    "    \n",
    "    # Join\n",
    "    joined_df = sales_df.join(products_df, \"product_id\", \"left\") \\\n",
    "                        .join(stores_df, \"store_id\", \"left\")\n",
    "    \n",
    "    # Derived columns\n",
    "    final_df = joined_df.withColumn(\"gross_amount\", expr(\"quantity * unit_price\")) \\\n",
    "        .withColumn(\"net_amount\", expr(\"gross_amount - discount\")) \\\n",
    "        .withColumn(\"profit\", expr(\"net_amount - (quantity * cost_price)\"))\n",
    "    \n",
    "    log_message(\"Transformation completed successfully.\")\n",
    "except Exception as e:\n",
    "    log_message(f\"Transformation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65c86528-5bba-4e36-a1ed-40f45a1b3e91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 589 bytes.\nWrote 643 bytes.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    log_message(\"Storage started.\")\n",
    "    output_path = \"dbfs:/FileStore/data_pipeline/clean_data/cleaned_sales\"\n",
    "    final_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(output_path)\n",
    "    log_message(\"Storage completed successfully.\")\n",
    "except Exception as e:\n",
    "    log_message(f\"Storage failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90ef2c69-0da4-41b5-81de-bc72502b4506",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Truncated to first 500 bytes]\n'[2025-11-10 13:59:11] Ingestion started.\\n[2025-11-10 14:00:37] Transformation started.\\n[2025-11-10 14:00:37] Transformation completed successfully.\\n[2025-11-10 14:00:41] Storage started.\\n[2025-11-10 14:00:44] Storage completed successfully.\\n[2025-11-10 14:02:18] Transformation started.\\n[2025-11-10 14:02:19] Transformation completed successfully.\\n\\n[2025-11-10 14:02:22] Storage started.\\n[2025-11-10 14:02:25] Storage completed successfully.\\n\\n[2025-11-10 14:03:13] Transformation started.\\n[2025-11-10'"
     ]
    }
   ],
   "source": [
    "display(dbutils.fs.head(\"dbfs:/FileStore/data_pipeline/logs/log.txt\", 500))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Retail Analytics Task 2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}